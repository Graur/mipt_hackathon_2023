{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "import sqlite3\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем соединение с базой данных\n",
    "conn = sqlite3.connect('garpix_db_0.0.2.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calculation_id  size_width  size_height  size_length  density_percent  \\\n",
      "0           16900        98.0        143.0        385.0         62.12352   \n",
      "1           16900       196.0        198.0        363.0         62.12352   \n",
      "2           16900       277.0        190.0        407.0         62.12352   \n",
      "3           16900       170.0        325.0        255.0         62.12352   \n",
      "4           16900       250.0        350.0        160.0         62.12352   \n",
      "\n",
      "   filling_space_percent  \n",
      "0               61.22618  \n",
      "1               61.22618  \n",
      "2               61.22618  \n",
      "3               61.22618  \n",
      "4               61.22618  \n"
     ]
    }
   ],
   "source": [
    "# Формируем DataFrame  \n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT\n",
    "        dr.calculation_id,\n",
    "\t\tb.size_width, \n",
    "\t\tb.size_height, \n",
    "\t\tb.size_length,\n",
    "\t\tdr.density_percent,\n",
    "\t\tdr.filling_space_percent\n",
    "    FROM data_result dr\n",
    "    JOIN boxes b ON b.calculation_id = dr.calculation_id\n",
    "\"\"\", conn)\n",
    "print(df.head())\n",
    "\n",
    "# Группируем данные по calculate_id \n",
    "# Группируем DataFrame по calculate_id и создаем новый столбец с именем \"width_list\", \n",
    "# который содержит список всех значений size_width (ширина ящика) для каждой группы. \n",
    "grouped_df = df.groupby('calculation_id')['size_width'].apply(list).reset_index(name='width_list')\n",
    "# Наконец, создаем список тензоров из этих списков значений size_width.\n",
    "data = [torch.tensor(widths).unsqueeze(1) for widths in grouped_df['width_list']]\n",
    "\n",
    "# Формируем список значений процента заполнения грузового пространства\n",
    "# Группируем по calculation_id и извлекаем значения filling_space_percent \n",
    "grouped = df.groupby('calculation_id')['filling_space_percent'].unique()\n",
    "\n",
    "# Создаем np.array уникальных значений filling_space_percent для каждого calculation_id\n",
    "result = [[x] for x in grouped.values]\n",
    "rand_tmp = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/graur/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([628])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 3343.9140625\n",
      "Epoch 10, Loss 3330.6708984375\n",
      "Epoch 20, Loss 3315.8056640625\n",
      "Epoch 30, Loss 3297.2392578125\n",
      "Epoch 40, Loss 3270.699462890625\n",
      "Epoch 50, Loss 3227.579833984375\n",
      "Epoch 60, Loss 3167.64208984375\n",
      "Epoch 70, Loss 3113.173583984375\n",
      "Epoch 80, Loss 3070.112548828125\n",
      "Epoch 90, Loss 3034.38427734375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Определяется модель Net - это нейронная сеть, \n",
    "# состоящая из слоя RNN (рекуррентная нейронная сеть) и полносвязного слоя. \n",
    "# Скрытый размерность RNN равен 16, а входной размерность равна 1.\n",
    "# Модель использует функцию активации по умолчанию.\n",
    "class Net(nn.Module):\n",
    "# В конструкторе класса мы определяем наши слои: nn.RNN и nn.Linear. \n",
    "# Эти слои будут использоваться при проходе данных через модель. \n",
    "# Здесь мы указываем, что на входе имеем 1 признак, \n",
    "# размер скрытого состояния RNN равен 16, \n",
    "# а архитектура сети предполагает использование batch_first=True, \n",
    "# то есть размерность первой оси является размером пакета.\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=16, batch_first=True)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "\n",
    "# Функция forward принимает входной тензор x, \n",
    "# пропускает его через слой RNN, \n",
    "# получает выходной тензор и пропускает его через полносвязный слой, \n",
    "# чтобы получить выходной тензор.\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        x = self.fc(output[:, -1, :])\n",
    "        return x.squeeze()  # удаляем избыточные размерности\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# определяется устройство, на котором будет проходить обучение модели (GPU или CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "# Определяется функция потерь (criterion) - это среднеквадратичная ошибка (MSE).\n",
    "criterion = nn.MSELoss()\n",
    "# Определяется оптимизатор (optimizer) - \n",
    "# это алгоритм Adam с коэффициентом обучения 0.001, \n",
    "# который будет использоваться для обновления параметров модели.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Генерация данных для обучения\n",
    "# TODO тут вместо даты, нужно подставить наши значения (DONE)\n",
    "import numpy as np\n",
    "#num_samples = 1000\n",
    "#data = []\n",
    "#for i in range(num_samples):\n",
    "#    input_size = np.random.randint(3, 10)\n",
    "#    data.append(torch.from_numpy(np.random.rand(input_size, 1) * 100).float().to(device))\n",
    "# Генерируется синтетический набор данных для обучения - это список data, \n",
    "# который состоит из 1000 тензоров, \n",
    "# каждый из которых имеет случайный размер от 3 до 10 и \n",
    "# содержит случайные числа от 0 до 100. \n",
    "# Каждый тензор data заполняется нулями до максимальной длины, \n",
    "# чтобы все тензоры имели одинаковую длину. \n",
    "# Кроме того, создается тензор меток labels, \n",
    "# который состоит из 1000 случайных чисел от 0 до 100.\n",
    "data_tensor = pad_sequence(data, batch_first=True)  # заполняем более короткие тензоры нулями\n",
    "# np.random.rand(num_samples, 1) создает массив размером (num_samples, 1) \n",
    "# со случайными значениями в диапазоне от 0 до 1.\n",
    "# .to(device) перемещает тензор на устройство, указанное в переменной device, если оно доступно.\n",
    "# TODO сюда вместо rand_tmp вставляем значени, что хотим получить  (DONE)\n",
    "#rand_tmp = np.random.rand(num_samples, 1) * 100\n",
    "# перемещение labels на устройство обусловлено тем, \n",
    "# что он используется в вычислении функции потерь, \n",
    "# которая определена на устройстве.\n",
    "labels = torch.from_numpy(rand_tmp).float().to(device)\n",
    "# изменяем размерность целевого тензора\n",
    "# Аргумент -1 в данном случае означает, \n",
    "# что размерность должна быть вычислена автоматически \n",
    "# на основе других размерностей тензора.\n",
    "labels = labels.view(-1)\n",
    "\n",
    "\n",
    "# обучение модели - модель пропускает каждый тензор из data через себя, \n",
    "# получая предсказание output, \n",
    "# затем сравнивает его с соответствующей меткой target, \n",
    "# используя функцию потерь criterion, и вычисляет ошибку loss. \n",
    "# Далее выполняется обратное распространение ошибки и обновление весов optimizer.step(). \n",
    "# Процесс обучения повторяется в цикле 100 раз.\n",
    "for epoch in range(100):\n",
    "# Обнуление градиента всех параметров модели. \n",
    "# Это необходимо делать перед каждым проходом (эпохой) обучения модели, \n",
    "# чтобы избежать накопления градиентов от предыдущих проходов. \n",
    "# Без этого может произойти неожиданное обновление весов, \n",
    "# которое может ухудшить качество модели.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Вызываем метод forward модели model с тензором data_tensor в качестве входных данных\n",
    "    output = model(data_tensor)\n",
    "    target = labels\n",
    "\n",
    "# Вычисляем функцию потерь между предсказаниями модели output[-1] и истинными значениями target.\n",
    "    loss = criterion(output[-1], target)\n",
    "\n",
    "# Выполняем обратное распространение ошибки и обновляем веса\n",
    "\t# выполняет обратное распространение ошибки (backpropagation) в нейронной сети. \n",
    "\t# Она вычисляет градиенты функции потерь по всем параметрам модели\n",
    "    loss.backward()\n",
    "\t# обновление весов модели с помощью оптимизатора \n",
    "    optimizer.step()\n",
    "\n",
    "# loss.item() — значению функции потерь на данной эпохе. \n",
    "# Функция потерь (MSE) измеряет разницу между прогнозируемым значением и \n",
    "# фактическим значением, поэтому чем меньше значение функции потерь, \n",
    "# тем лучше модель обучена на данном этапе\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {}, Loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
